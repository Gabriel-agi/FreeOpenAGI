<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <!-- Title set by JS -->
    <title data-translate="page_title_news">AI News - Road to Free Open AGI</title>
    <style>
        :root {
            --primary: #6e48aa;
            --secondary: #9d50bb;
            --accent: #4776e6;
            --dark: #1a1a2e;
            --light: #f8f9fa;
            --success: #4cc9f0;
            --text-light: #f8f9fa;
            --post-bg: rgba(255,255,255,0.05);
            --post-border: rgba(255,255,255,0.1);
        }
        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
            font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif;
        }
        body {
            background: linear-gradient(135deg, var(--dark), #16213e);
            color: var(--text-light);
            line-height: 1.6;
            overflow-x: hidden;
            min-height: 100vh;
            display: flex; /* For sticky footer */
            flex-direction: column;
        }
        .particles {
            position: fixed;
            top: 0;
            left: 0;
            width: 100%;
            height: 100%;
            z-index: -1;
            pointer-events: none;
        }
        .container {
            max-width: 800px;
            margin: 0 auto;
            padding: 2rem;
            flex-grow: 1; /* Allow content to grow */
        }
        /* --- Navigation & Language Switcher Styles --- */
        .main-nav {
            display: flex;
            align-items: center;
            gap: 1.5rem;
            padding: 1rem;
            margin-bottom: 2rem;
            flex-wrap: wrap;
            background: rgba(255,255,255,0.05);
            border-radius: 10px;
            border: 1px solid rgba(255,255,255,0.1);
        }
        .main-nav a {
            color: var(--success);
            text-decoration: none;
            font-weight: 500;
            padding: 0.5rem 1rem;
            border-radius: 5px;
            transition: all 0.3s ease;
            white-space: nowrap;
        }
        .main-nav a:hover {
            background: rgba(76, 201, 240, 0.2);
            transform: translateY(-2px);
        }
        .language-switcher {
            margin-left: auto;
            position: relative;
        }
        #languageSelect {
            background: rgba(255,255,255,0.1);
            color: white;
            border: 1px solid var(--success);
            border-radius: 5px;
            padding: 6px 30px 6px 12px;
            cursor: pointer;
            font-size: 0.9rem;
            appearance: none;
            -webkit-appearance: none;
            -moz-appearance: none;
            background-image: url('data:image/svg+xml;charset=US-ASCII,%3Csvg%20xmlns%3D%22http%3A%2F%2Fwww.w3.org%2F2000%2Fsvg%22%20width%3D%22292.4%22%20height%3D%22292.4%22%3E%3Cpath%20fill%3D%22%234CC9F0%22%20d%3D%22M287%2069.4a17.6%2017.6%200%200%200-13-5.4H18.4c-5%200-9.3%201.8-12.9%205.4A17.6%2017.6%200%200%200%200%2082.2c0%205%201.8%209.3%205.4%2012.9l128%20127.9c3.6%203.6%207.8%205.4%2012.8%205.4s9.2-1.8%2012.8-5.4L287%2095c3.5-3.5%205.4-7.8%205.4-12.8%200-5-1.9-9.2-5.5-12.8z%22%2F%3E%3C%2Fsvg%3E');
            background-repeat: no-repeat;
            background-position: right 10px center;
            background-size: 10px auto;
            transition: background-color 0.3s ease;
        }
        #languageSelect:hover {
             background-color: rgba(255,255,255,0.15);
        }
        #languageSelect option {
             background: var(--dark);
             color: var(--text-light);
        }
        /* --- End Nav & Switcher --- */

        /* News Post Styles */
        .news-container {
            margin-top: 2rem;
            animation: fadeIn 1.5s ease-out;
        }

        .post {
            background: var(--post-bg);
            border-radius: 10px;
            border: 1px solid var(--post-border);
            overflow: hidden;
            transition: all 0.3s ease;
            margin-bottom: 1.5rem;
        }

        .post:hover {
            transform: translateY(-3px);
            box-shadow: 0 10px 20px rgba(0,0,0,0.2);
            background: rgba(255,255,255,0.08);
        }

        .post-content {
            padding: 1.5rem;
        }

        .post-title {
            font-size: 1.4rem;
            margin-bottom: 0.5rem;
            color: var(--success);
        }

        .post-meta {
            display: flex;
            flex-wrap: wrap; /* Allow wrapping */
            gap: 0.5rem 1rem; /* row gap, column gap */
            font-size: 0.9rem;
            color: rgba(255,255,255,0.7);
            margin-bottom: 1rem;
        }
         .post-meta span {
             white-space: nowrap;
         }

        .post-text {
            margin-bottom: 1rem;
            line-height: 1.6;
            display: -webkit-box;
            -webkit-line-clamp: 3; /* Limit to 3 lines */
            -webkit-box-orient: vertical;
            overflow: hidden;
            text-overflow: ellipsis;
        }

        .post-text.expanded {
            -webkit-line-clamp: unset; /* Show all lines when expanded */
        }

        .post-links {
            margin-top: 1rem;
            display: none; /* Hidden by default */
            border-top: 1px solid var(--post-border); /* Separator */
            padding-top: 1rem;
        }

        .post-links.expanded {
            display: block; /* Shown when expanded */
        }
         .post-links strong { /* Style the label */
            display: block;
            margin-bottom: 0.8rem;
            color: var(--success);
            font-weight: 500;
         }

        .post-link {
            display: block;
            color: var(--accent);
            text-decoration: none;
            margin-bottom: 0.5rem;
            word-break: break-all;
            transition: color 0.2s ease;
        }

        .post-link:hover {
            color: var(--success);
            text-decoration: underline;
        }

        .read-more {
            color: var(--accent);
            cursor: pointer;
            font-weight: 500;
            display: inline-block;
            margin-top: 0.5rem;
            transition: color 0.2s ease;
        }

        .read-more:hover {
            text-decoration: underline;
            color: var(--success);
        }

        /* Footer styles */
        footer {
            text-align: center;
            margin-top: auto;
            padding: 2rem;
            font-size: 0.9rem;
            opacity: 0.7;
            background: rgba(255,255,255,0.02);
        }

        /* Animations */
        @keyframes fadeIn {
            from { opacity: 0; }
            to { opacity: 1; }
        }

        /* Responsive */
        @media screen and (max-width: 768px) {
            .container {
                padding: 1rem;
            }
             .main-nav {
                 justify-content: center;
                 gap: 0.8rem;
                 padding: 0.8rem;
             }
             .main-nav a {
                 padding: 0.4rem 0.8rem;
                 font-size: 0.9rem;
             }
             .language-switcher {
                  margin-left: 0;
                  margin-top: 1rem;
             }
            .post-title {
                font-size: 1.2rem;
            }
        }
    </style>
</head>
<body>
    <div id="particles-js" class="particles"></div>
    <div class="container">
        <nav class="main-nav">
            <!-- Navigation Links -->
            <a href="index.html" data-translate="nav_home">Road to AGI</a>
            <a href="news.html" data-translate="nav_news">AI News</a>
            <a href="courses.html" data-translate="nav_courses">Courses</a>
            <a href="mission.html" data-translate="nav_mission">Mission</a>
            <a href="games.html" data-translate="nav_games">Games</a>
            <a href="comics.html" data-translate="nav_comics">Comics</a>
            <a href="videos.html" data-translate="nav_videos">Videos</a>
            <a href="apps.html" data-translate="nav_apps">Apps</a>
            <a href="contact.html" data-translate="nav_contact">Contact</a>
            <a href="about.html" data-translate="nav_about">About</a>
            <!-- Language Switcher -->
            <div class="language-switcher">
                <select id="languageSelect">
                    <option value="en">🇬🇧 English</option>
                    <option value="pt">🇧🇷 Português</option>
                    <option value="zh">🇨🇳 中文</option>
                </select>
            </div>
        </nav>

        <div class="news-container">
            <!-- News Post 1: Llama 4 (Newest - Now First) -->
            <div class="post">
                <div class="post-content">
                    <h3 class="post-title" data-translate="news_post2_title">Meta Releases Llama 4: Multimodal Capabilities Emerge</h3>
                    <div class="post-meta">
                        <span><span data-translate="news_posted_by">Posted by</span> Gabriel</span>
                        <span>13/04/2025</span> <!-- Updated Date -->
                    </div>
                    <p class="post-text" data-translate="news_post2_text">Meta has announced the release of Llama 4, a new iteration in their open-source model series, notably featuring multimodal capabilities (text, image, potentially audio/video understanding). While this release expands the open-source ecosystem, initial assessments suggest it doesn't represent a fundamental leap compared to leading models like DeepSeek V3. Consequently, the progress towards AGI remains estimated at 39%. Llama 4 is a valuable contribution, but not seen as significantly altering the AGI timeline currently.</p>
                    <span class="read-more" data-translate="news_read_more">Read more...</span>
                    <div class="post-links">
                        <strong data-translate="news_related_links">Related Links:</strong>
                        <!-- Updated Link Section for Llama 4 -->
                        <a href="https://ai.meta.com/blog/llama-4-multimodal-intelligence/" class="post-link" target="_blank" data-translate="news_post2_link_blog">Official Blog Post: Llama 4 Multimodal Intelligence</a>
                    </div>
                </div>
            </div>

             <!-- News Post 2: Qwen (Older - Now Second) -->
            <div class="post">
                <div class="post-content">
                    <h3 class="post-title" data-translate="news_post1_title">Qwen2.5-Omni: First Open Source Omni Model Released</h3>
                    <div class="post-meta">
                        <span><span data-translate="news_posted_by">Posted by</span> Gabriel</span>
                        <span>28/03/2025</span>
                    </div>
                    <p class="post-text" data-translate="news_post1_text">Qwen has introduced voice and video chat capabilities in Qwen Chat, allowing users to interact with the AI like a phone or video call. The team has open-sourced Qwen2.5-Omni-7B under Apache 2.0 license, releasing both the model and technical details. This is an omni model - a single model that understands text, audio, images, and video while outputting text and audio. It features a "thinker-talker" architecture enabling simultaneous thinking and talking. The team believes AGI will be agent-based on omni models.</p>
                    <span class="read-more" data-translate="news_read_more">Read more...</span>
                    <div class="post-links">
                        <strong data-translate="news_related_links">Related Links:</strong>
                        <a href="https://chat.qwenlm.ai" class="post-link" target="_blank">Qwen Chat Demo</a>
                        <a href="https://youtube.com/watch?v=yKcANdkRuNI" class="post-link" target="_blank">Video Demo</a>
                        <a href="https://github.com/QwenLM/Qwen2.5-Omni/blob/main/assets/Qwen2.5_Omni.pdf" class="post-link" target="_blank">Technical Paper</a>
                        <a href="https://qwenlm.github.io/blog/qwen2.5-omni" class="post-link" target="_blank">Blog Post</a>
                        <a href="https://github.com/QwenLM/Qwen2.5-Omni" class="post-link" target="_blank">GitHub Repository</a>
                        <a href="https://huggingface.co/Qwen/Qwen2.5-Omni-7B" class="post-link" target="_blank">Hugging Face Model</a>
                        <a href="https://modelscope.cn/models/Qwen/Qwen2.5-Omni-7B" class="post-link" target="_blank">ModelScope</a>
                    </div>
                </div>
            </div>

            <!-- Add more .post divs here for future news items -->

        </div> <!-- End .news-container -->
    </div> <!-- End .container -->

     <!-- Footer -->
    <footer>
        <p data-translate="footer_1">The Road to Free Open AGI Project | Committed to ethical, open artificial intelligence for all</p>
        <p data-translate="footer_2_static">Building tools and experiences for an open AI future.</p>
    </footer>

    <script src="https://cdn.jsdelivr.net/particles.js/2.0.0/particles.min.js"></script>
    <script>
        // Initialize particles.js
        particlesJS("particles-js", { /* ... particles config ... */
             "particles": { "number": { "value": 80, "density": { "enable": true, "value_area": 800 } }, "color": { "value": "#9d50bb" }, "shape": { "type": "circle", "stroke": { "width": 0, "color": "#000000" }, "polygon": { "nb_sides": 5 } }, "opacity": { "value": 0.5, "random": false, "anim": { "enable": false, "speed": 1, "opacity_min": 0.1, "sync": false } }, "size": { "value": 3, "random": true, "anim": { "enable": false, "speed": 40, "size_min": 0.1, "sync": false } }, "line_linked": { "enable": true, "distance": 150, "color": "#6e48aa", "opacity": 0.4, "width": 1 }, "move": { "enable": true, "speed": 2, "direction": "none", "random": false, "straight": false, "out_mode": "out", "bounce": false, "attract": { "enable": false, "rotateX": 600, "rotateY": 1200 } } }, "interactivity": { "detect_on": "canvas", "events": { "onhover": { "enable": true, "mode": "grab" }, "onclick": { "enable": true, "mode": "push" }, "resize": true }, "modes": { "grab": { "distance": 140, "line_linked": { "opacity": 1 } }, "bubble": { "distance": 400, "size": 40, "duration": 2, "opacity": 8, "speed": 3 }, "repulse": { "distance": 200, "duration": 0.4 }, "push": { "particles_nb": 4 }, "remove": { "particles_nb": 2 } } }, "retina_detect": true
        });

        // Translation Script
        const translations = {
            en: {
                // Page Meta
                "page_title_news": "AI News - Road to Free Open AGI",
                // Navigation
                "nav_home": "Road to AGI", "nav_news": "AI News", "nav_courses": "Courses", "nav_mission": "Mission", "nav_games": "Games", "nav_comics": "Comics", "nav_videos": "Videos", "nav_apps": "Apps", "nav_contact": "Contact", "nav_about": "About",
                // Shared
                "news_posted_by": "Posted by",
                "news_read_more": "Read more...",
                "news_read_less": "Read less...",
                "news_related_links": "Related Links:",
                 // News Post 1 (Qwen - Now Second)
                "news_post1_title": "Qwen2.5-Omni: First Open Source Omni Model Released",
                "news_post1_text": "Qwen has introduced voice and video chat capabilities in Qwen Chat, allowing users to interact with the AI like a phone or video call. The team has open-sourced Qwen2.5-Omni-7B under Apache 2.0 license, releasing both the model and technical details. This is an omni model - a single model that understands text, audio, images, and video while outputting text and audio. It features a \"thinker-talker\" architecture enabling simultaneous thinking and talking. The team believes AGI will be agent-based on omni models.",
                 // News Post 2 (Llama 4 - Now First)
                "news_post2_title": "Meta Releases Llama 4: Multimodal Capabilities Emerge",
                "news_post2_text": "Meta has announced the release of Llama 4, a new iteration in their open-source model series, notably featuring multimodal capabilities (text, image, potentially audio/video understanding). While this release expands the open-source ecosystem, initial assessments suggest it doesn't represent a fundamental leap compared to leading models like DeepSeek V3. Consequently, the progress towards AGI remains estimated at 39%. Llama 4 is a valuable contribution, but not seen as significantly altering the AGI timeline currently.",
                "news_post2_link_blog": "Official Blog Post: Llama 4 Multimodal Intelligence", // NEW link text key
                // Footer
                "footer_1": "The Road to Free Open AGI Project | Committed to ethical, open artificial intelligence for all",
                "footer_2_static": "Building tools and experiences for an open AI future."
            },
            pt: {
                 // Page Meta
                "page_title_news": "Notícias de IA - Caminho para AGI Gratuita e Aberta",
                // Navigation
                "nav_home": "Caminho para AGI", "nav_news": "Notícias de IA", "nav_courses": "Cursos", "nav_mission": "Missão", "nav_games": "Jogos", "nav_comics": "Quadrinhos", "nav_videos": "Vídeos", "nav_apps": "Aplicativos", "nav_contact": "Contato", "nav_about": "Sobre",
                // Shared
                "news_posted_by": "Postado por",
                "news_read_more": "Leia mais...",
                "news_read_less": "Leia menos...",
                "news_related_links": "Links Relacionados:",
                 // News Post 1 (Qwen - Now Second)
                "news_post1_title": "Qwen2.5-Omni: Primeiro Modelo Omni de Código Aberto Lançado",
                "news_post1_text": "Qwen introduziu capacidades de chat por voz e vídeo no Qwen Chat, permitindo aos usuários interagir com a IA como numa chamada telefônica ou de vídeo. A equipe tornou open-source o Qwen2.5-Omni-7B sob a licença Apache 2.0, liberando tanto o modelo quanto os detalhes técnicos. Este é um modelo omni - um único modelo que entende texto, áudio, imagens e vídeo, enquanto gera texto e áudio. Ele apresenta uma arquitetura \"thinker-talker\" que permite pensar e falar simultaneamente. A equipe acredita que a AGI será baseada em agentes sobre modelos omni.",
                 // News Post 2 (Llama 4 - Now First)
                "news_post2_title": "Meta Lança Llama 4: Emergência de Capacidades Multimodais",
                "news_post2_text": "A Meta anunciou o lançamento do Llama 4, uma nova iteração em sua série de modelos de código aberto, notavelmente apresentando capacidades multimodais (compreensão de texto, imagem, potencialmente áudio/vídeo). Embora este lançamento expanda o ecossistema de código aberto, avaliações iniciais sugerem que ele não representa um salto fundamental em comparação com modelos líderes como o DeepSeek V3. Consequentemente, o progresso em direção à AGI permanece estimado em 39%. O Llama 4 é uma contribuição valiosa, mas não vista como alterando significativamente a linha do tempo da AGI no momento.",
                "news_post2_link_blog": "Postagem Oficial do Blog: Llama 4 Inteligência Multimodal", // NEW link text key
                 // Footer
                "footer_1": "O Projeto Caminho para AGI Gratuita e Aberta | Comprometido com inteligência artificial ética e aberta para todos",
                "footer_2_static": "Construindo ferramentas e experiências para um futuro de IA aberta."
            },
            zh: {
                 // Page Meta
                "page_title_news": "AI新闻 - 通往自由开放AGI之路",
                // Navigation
                "nav_home": "通往AGI之路", "nav_news": "AI新闻", "nav_courses": "课程", "nav_mission": "使命", "nav_games": "游戏", "nav_comics": "漫画", "nav_videos": "视频", "nav_apps": "应用", "nav_contact": "联系", "nav_about": "关于",
                // Shared
                "news_posted_by": "发布者：",
                "news_read_more": "阅读更多...",
                "news_read_less": "收起...",
                "news_related_links": "相关链接：",
                  // News Post 1 (Qwen - Now Second)
                "news_post1_title": "Qwen2.5-Omni：首个开源Omni模型发布",
                "news_post1_text": "Qwen 在 Qwen Chat 中引入了语音和视频聊天功能，允许用户像进行电话或视频通话一样与 AI 互动。团队已根据 Apache 2.0 许可证开源了 Qwen2.5-Omni-7B，发布了模型和技术细节。这是一个 omni 模型——一个能够理解文本、音频、图像和视频，同时输出文本和音频的单一模型。它采用了“思考者-说话者”架构，能够同时思考和说话。该团队认为AGI将是基于omni模型的智能体。",
                 // News Post 2 (Llama 4 - Now First)
                "news_post2_title": "Meta发布Llama 4：多模态能力显现",
                "news_post2_text": "Meta 宣布发布 Llama 4，这是其开源模型系列的新迭代，显著特点是具备多模态能力（文本、图像，可能还有音频/视频理解）。虽然此次发布扩展了开源生态系统，但初步评估表明，与 DeepSeek V3 等领先模型相比，它并未代表基础能力的根本性飞跃。因此，通往 AGI 的进展估计仍为 39%。Llama 4 是一项宝贵的贡献，但目前未被视为能显著改变 AGI 时间线的突破。",
                 "news_post2_link_blog": "官方博客文章：Llama 4 多模态智能", // NEW link text key
                 // Footer
                "footer_1": "自由开放AGI之路项目 | 致力于为所有人提供道德、开放的人工智能",
                "footer_2_static": "为开放AI的未来构建工具和体验。"
            }
        };

        // Language management functions (Copied)
        function getStoredLanguage() {
             const storedLang = localStorage.getItem('userLanguage');
             if (storedLang && translations[storedLang]) return storedLang;
             const browserLang = navigator.language || navigator.userLanguage;
             if (browserLang.startsWith('pt')) return 'pt';
             if (browserLang.startsWith('zh')) return 'zh';
             return 'en';
        }

        function setLanguage(lang) {
             if (!translations[lang]) lang = 'en';
             localStorage.setItem('userLanguage', lang);
             document.documentElement.lang = lang;
             translatePage(lang);
        }

        function translatePage(lang) {
            if (!translations[lang]) lang = 'en';
            const langSelect = document.getElementById('languageSelect');
            if (langSelect) langSelect.value = lang;

            // Translate page title
            const pageTitleElement = document.querySelector('title[data-translate="page_title_news"]');
            if (pageTitleElement && translations[lang]["page_title_news"]) {
                pageTitleElement.textContent = translations[lang]["page_title_news"];
            }

            // Translate all elements with data-translate attribute
            document.querySelectorAll('[data-translate]').forEach(el => {
                if (el.tagName === 'TITLE') return;
                const key = el.getAttribute('data-translate');
                if (translations[lang] && translations[lang][key]) {
                     // Special check for the read-more button to use appropriate text based on state
                     if (el.classList.contains('read-more')) {
                         const postContent = el.closest('.post-content'); // Find parent content block
                         const postText = postContent?.querySelector('.post-text'); // Find text within that block
                         if (postText && postText.classList.contains('expanded')) {
                            el.textContent = translations[lang]['news_read_less'] || translations['en']['news_read_less'];
                         } else {
                            el.textContent = translations[lang]['news_read_more'] || translations['en']['news_read_more'];
                         }
                     } else {
                        el.textContent = translations[lang][key];
                     }
                }
            });

            // Translate placeholders
            document.querySelectorAll('[data-translate-placeholder]').forEach(el => { /* ... */ });
        }

        // Run on page load
        document.addEventListener('DOMContentLoaded', function() {
            // --- Initialize Language ---
            const initialLang = getStoredLanguage();
            setLanguage(initialLang); // Apply initial language

            // --- Setup Language Switcher ---
            const langSelect = document.getElementById('languageSelect');
            if (langSelect) {
                langSelect.addEventListener('change', (e) => {
                    setLanguage(e.target.value);
                });
            }

            // --- Read more functionality (Needs to work for all posts) ---
             // Get all read-more buttons *after* content is loaded
             const readMoreButtons = document.querySelectorAll('.read-more');

             readMoreButtons.forEach(button => {
                 // Find the associated elements relative to the button
                 const postContent = button.closest('.post-content');
                 if (!postContent) return; // Skip if structure is wrong

                 const postText = postContent.querySelector('.post-text');
                 const postLinks = postContent.querySelector('.post-links');

                 if (!postText || !postLinks) return; // Skip if elements not found

                 button.addEventListener('click', function() {
                     postText.classList.toggle('expanded');
                     postLinks.classList.toggle('expanded');

                     // Update button text using the current language
                     const currentLang = getStoredLanguage(); // Re-fetch current language setting
                     if (postText.classList.contains('expanded')) {
                         this.textContent = translations[currentLang]?.['news_read_less'] || translations['en']['news_read_less'];
                     } else {
                         this.textContent = translations[currentLang]?.['news_read_more'] || translations['en']['news_read_more'];
                     }
                 });
             });
             // --- End Read more ---

        }); // End DOMContentLoaded
    </script>
</body>
</html>
