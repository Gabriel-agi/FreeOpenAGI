<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <!-- Title set by JS -->
    <title data-translate="page_title_news">AI News - Road to Free Open AGI</title>
    <style>
        :root {
            --primary: #6e48aa;
            --secondary: #9d50bb;
            --accent: #4776e6;
            --dark: #1a1a2e;
            --light: #f8f9fa;
            --success: #4cc9f0;
            --text-light: #f8f9fa;
            --post-bg: rgba(255,255,255,0.05);
            --post-border: rgba(255,255,255,0.1);
        }
        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
            font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif;
        }
        body {
            background: linear-gradient(135deg, var(--dark), #16213e);
            color: var(--text-light);
            line-height: 1.6;
            overflow-x: hidden;
            min-height: 100vh;
            display: flex; /* For sticky footer */
            flex-direction: column;
        }
        .particles {
            position: fixed;
            top: 0;
            left: 0;
            width: 100%;
            height: 100%;
            z-index: -1;
            pointer-events: none;
        }
        .container {
            max-width: 800px;
            margin: 0 auto;
            padding: 2rem;
            flex-grow: 1; /* Allow content to grow */
        }
        /* --- Navigation & Language Switcher Styles --- */
        .main-nav {
            display: flex;
            align-items: center;
            gap: 1.5rem;
            padding: 1rem;
            margin-bottom: 2rem;
            flex-wrap: wrap;
            background: rgba(255,255,255,0.05);
            border-radius: 10px;
            border: 1px solid rgba(255,255,255,0.1);
        }
        .main-nav a {
            color: var(--success);
            text-decoration: none;
            font-weight: 500;
            padding: 0.5rem 1rem;
            border-radius: 5px;
            transition: all 0.3s ease;
            white-space: nowrap;
        }
        .main-nav a:hover {
            background: rgba(76, 201, 240, 0.2);
            transform: translateY(-2px);
        }
        .language-switcher {
            margin-left: auto;
            position: relative;
        }
        #languageSelect {
            background: rgba(255,255,255,0.1);
            color: white;
            border: 1px solid var(--success);
            border-radius: 5px;
            padding: 6px 30px 6px 12px;
            cursor: pointer;
            font-size: 0.9rem;
            appearance: none;
            -webkit-appearance: none;
            -moz-appearance: none;
            background-image: url('data:image/svg+xml;charset=US-ASCII,%3Csvg%20xmlns%3D%22http%3A%2F%2Fwww.w3.org%2F2000%2Fsvg%22%20width%3D%22292.4%22%20height%3D%22292.4%22%3E%3Cpath%20fill%3D%22%234CC9F0%22%20d%3D%22M287%2069.4a17.6%2017.6%200%200%200-13-5.4H18.4c-5%200-9.3%201.8-12.9%205.4A17.6%2017.6%200%200%200%200%2082.2c0%205%201.8%209.3%205.4%2012.9l128%20127.9c3.6%203.6%207.8%205.4%2012.8%205.4s9.2-1.8%2012.8-5.4L287%2095c3.5-3.5%205.4-7.8%205.4-12.8%200-5-1.9-9.2-5.5-12.8z%22%2F%3E%3C%2Fsvg%3E');
            background-repeat: no-repeat;
            background-position: right 10px center;
            background-size: 10px auto;
            transition: background-color 0.3s ease;
        }
        #languageSelect:hover {
             background-color: rgba(255,255,255,0.15);
        }
        #languageSelect option {
             background: var(--dark);
             color: var(--text-light);
        }
        /* --- End Nav & Switcher --- */

        /* News Post Styles */
        .news-container {
            margin-top: 2rem;
            animation: fadeIn 1.5s ease-out;
        }

        .post {
            background: var(--post-bg);
            border-radius: 10px;
            border: 1px solid var(--post-border);
            overflow: hidden;
            transition: all 0.3s ease;
            margin-bottom: 1.5rem;
        }

        .post:hover {
            transform: translateY(-3px);
            box-shadow: 0 10px 20px rgba(0,0,0,0.2);
            background: rgba(255,255,255,0.08);
        }

        .post-content {
            padding: 1.5rem;
        }

        .post-title {
            font-size: 1.4rem;
            margin-bottom: 0.5rem;
            color: var(--success);
        }

        .post-meta {
            display: flex;
            flex-wrap: wrap; /* Allow wrapping */
            gap: 0.5rem 1rem; /* row gap, column gap */
            font-size: 0.9rem;
            color: rgba(255,255,255,0.7);
            margin-bottom: 1rem;
        }
         .post-meta span {
             white-space: nowrap;
         }

        .post-text {
            margin-bottom: 1rem;
            line-height: 1.6;
            display: -webkit-box;
            -webkit-line-clamp: 3; /* Limit to 3 lines */
            -webkit-box-orient: vertical;
            overflow: hidden;
            text-overflow: ellipsis;
        }

        .post-text.expanded {
            -webkit-line-clamp: unset; /* Show all lines when expanded */
        }

        .post-links {
            margin-top: 1rem;
            display: none; /* Hidden by default */
            border-top: 1px solid var(--post-border); /* Separator */
            padding-top: 1rem;
        }

        .post-links.expanded {
            display: block; /* Shown when expanded */
        }
         .post-links strong { /* Style the label */
            display: block;
            margin-bottom: 0.8rem;
            color: var(--success);
            font-weight: 500;
         }

        .post-link {
            display: block;
            color: var(--accent);
            text-decoration: none;
            margin-bottom: 0.5rem;
            word-break: break-all;
            transition: color 0.2s ease;
        }

        .post-link:hover {
            color: var(--success);
            text-decoration: underline;
        }

        .read-more {
            color: var(--accent);
            cursor: pointer;
            font-weight: 500;
            display: inline-block;
            margin-top: 0.5rem;
            transition: color 0.2s ease;
        }

        .read-more:hover {
            text-decoration: underline;
            color: var(--success);
        }

        /* Footer styles */
        footer {
            text-align: center;
            margin-top: auto;
            padding: 2rem;
            font-size: 0.9rem;
            opacity: 0.7;
            background: rgba(255,255,255,0.02);
        }

        /* Animations */
        @keyframes fadeIn {
            from { opacity: 0; }
            to { opacity: 1; }
        }

        /* Responsive */
        @media screen and (max-width: 768px) {
            .container {
                padding: 1rem;
            }
             .main-nav {
                 justify-content: center;
                 gap: 0.8rem;
                 padding: 0.8rem;
             }
             .main-nav a {
                 padding: 0.4rem 0.8rem;
                 font-size: 0.9rem;
             }
             .language-switcher {
                  margin-left: 0;
                  margin-top: 1rem;
             }
            .post-title {
                font-size: 1.2rem;
            }
        }
    </style>
</head>
<body>
    <div id="particles-js" class="particles"></div>
    <div class="container">
        <nav class="main-nav">
            <!-- Navigation Links -->
            <a href="index.html" data-translate="nav_home">Road to AGI</a>
            <a href="news.html" data-translate="nav_news">AI News</a>
            <a href="courses.html" data-translate="nav_courses">Courses</a>
            <a href="mission.html" data-translate="nav_mission">Mission</a>
            <a href="games.html" data-translate="nav_games">Games</a>
            <a href="comics.html" data-translate="nav_comics">Comics</a>
            <a href="videos.html" data-translate="nav_videos">Videos</a>
            <a href="apps.html" data-translate="nav_apps">Apps</a>
            <a href="contact.html" data-translate="nav_contact">Contact</a>
            <a href="about.html" data-translate="nav_about">About</a>
            <!-- Language Switcher -->
            <div class="language-switcher">
                <select id="languageSelect">
                    <option value="en">ðŸ‡¬ðŸ‡§ English</option>
                    <option value="pt">ðŸ‡§ðŸ‡· PortuguÃªs</option>
                    <option value="zh">ðŸ‡¨ðŸ‡³ ä¸­æ–‡</option>
                </select>
            </div>
        </nav>

        <div class="news-container">
            <!-- News Post 1: Llama 4 (Newest - Now First) -->
            <div class="post">
                <div class="post-content">
                    <h3 class="post-title" data-translate="news_post2_title">Meta Releases Llama 4: Multimodal Capabilities Emerge</h3>
                    <div class="post-meta">
                        <span><span data-translate="news_posted_by">Posted by</span> Gabriel</span>
                        <span>13/04/2025</span> <!-- Updated Date -->
                    </div>
                    <p class="post-text" data-translate="news_post2_text">Meta has announced the release of Llama 4, a new iteration in their open-source model series, notably featuring multimodal capabilities (text, image, potentially audio/video understanding). While this release expands the open-source ecosystem, initial assessments suggest it doesn't represent a fundamental leap compared to leading models like DeepSeek V3. Consequently, the progress towards AGI remains estimated at 39%. Llama 4 is a valuable contribution, but not seen as significantly altering the AGI timeline currently.</p>
                    <span class="read-more" data-translate="news_read_more">Read more...</span>
                    <div class="post-links">
                        <strong data-translate="news_related_links">Related Links:</strong>
                        <!-- Updated Link Section for Llama 4 -->
                        <a href="https://ai.meta.com/blog/llama-4-multimodal-intelligence/" class="post-link" target="_blank" data-translate="news_post2_link_blog">Official Blog Post: Llama 4 Multimodal Intelligence</a>
                    </div>
                </div>
            </div>

             <!-- News Post 2: Qwen (Older - Now Second) -->
            <div class="post">
                <div class="post-content">
                    <h3 class="post-title" data-translate="news_post1_title">Qwen2.5-Omni: First Open Source Omni Model Released</h3>
                    <div class="post-meta">
                        <span><span data-translate="news_posted_by">Posted by</span> Gabriel</span>
                        <span>28/03/2025</span>
                    </div>
                    <p class="post-text" data-translate="news_post1_text">Qwen has introduced voice and video chat capabilities in Qwen Chat, allowing users to interact with the AI like a phone or video call. The team has open-sourced Qwen2.5-Omni-7B under Apache 2.0 license, releasing both the model and technical details. This is an omni model - a single model that understands text, audio, images, and video while outputting text and audio. It features a "thinker-talker" architecture enabling simultaneous thinking and talking. The team believes AGI will be agent-based on omni models.</p>
                    <span class="read-more" data-translate="news_read_more">Read more...</span>
                    <div class="post-links">
                        <strong data-translate="news_related_links">Related Links:</strong>
                        <a href="https://chat.qwenlm.ai" class="post-link" target="_blank">Qwen Chat Demo</a>
                        <a href="https://youtube.com/watch?v=yKcANdkRuNI" class="post-link" target="_blank">Video Demo</a>
                        <a href="https://github.com/QwenLM/Qwen2.5-Omni/blob/main/assets/Qwen2.5_Omni.pdf" class="post-link" target="_blank">Technical Paper</a>
                        <a href="https://qwenlm.github.io/blog/qwen2.5-omni" class="post-link" target="_blank">Blog Post</a>
                        <a href="https://github.com/QwenLM/Qwen2.5-Omni" class="post-link" target="_blank">GitHub Repository</a>
                        <a href="https://huggingface.co/Qwen/Qwen2.5-Omni-7B" class="post-link" target="_blank">Hugging Face Model</a>
                        <a href="https://modelscope.cn/models/Qwen/Qwen2.5-Omni-7B" class="post-link" target="_blank">ModelScope</a>
                    </div>
                </div>
            </div>

            <!-- Add more .post divs here for future news items -->

        </div> <!-- End .news-container -->
    </div> <!-- End .container -->

     <!-- Footer -->
    <footer>
        <p data-translate="footer_1">The Road to Free Open AGI Project | Committed to ethical, open artificial intelligence for all</p>
        <p data-translate="footer_2_static">Building tools and experiences for an open AI future.</p>
    </footer>

    <script src="https://cdn.jsdelivr.net/particles.js/2.0.0/particles.min.js"></script>
    <script>
        // Initialize particles.js
        particlesJS("particles-js", { /* ... particles config ... */
             "particles": { "number": { "value": 80, "density": { "enable": true, "value_area": 800 } }, "color": { "value": "#9d50bb" }, "shape": { "type": "circle", "stroke": { "width": 0, "color": "#000000" }, "polygon": { "nb_sides": 5 } }, "opacity": { "value": 0.5, "random": false, "anim": { "enable": false, "speed": 1, "opacity_min": 0.1, "sync": false } }, "size": { "value": 3, "random": true, "anim": { "enable": false, "speed": 40, "size_min": 0.1, "sync": false } }, "line_linked": { "enable": true, "distance": 150, "color": "#6e48aa", "opacity": 0.4, "width": 1 }, "move": { "enable": true, "speed": 2, "direction": "none", "random": false, "straight": false, "out_mode": "out", "bounce": false, "attract": { "enable": false, "rotateX": 600, "rotateY": 1200 } } }, "interactivity": { "detect_on": "canvas", "events": { "onhover": { "enable": true, "mode": "grab" }, "onclick": { "enable": true, "mode": "push" }, "resize": true }, "modes": { "grab": { "distance": 140, "line_linked": { "opacity": 1 } }, "bubble": { "distance": 400, "size": 40, "duration": 2, "opacity": 8, "speed": 3 }, "repulse": { "distance": 200, "duration": 0.4 }, "push": { "particles_nb": 4 }, "remove": { "particles_nb": 2 } } }, "retina_detect": true
        });

        // Translation Script
        const translations = {
            en: {
                // Page Meta
                "page_title_news": "AI News - Road to Free Open AGI",
                // Navigation
                "nav_home": "Road to AGI", "nav_news": "AI News", "nav_courses": "Courses", "nav_mission": "Mission", "nav_games": "Games", "nav_comics": "Comics", "nav_videos": "Videos", "nav_apps": "Apps", "nav_contact": "Contact", "nav_about": "About",
                // Shared
                "news_posted_by": "Posted by",
                "news_read_more": "Read more...",
                "news_read_less": "Read less...",
                "news_related_links": "Related Links:",
                 // News Post 1 (Qwen - Now Second)
                "news_post1_title": "Qwen2.5-Omni: First Open Source Omni Model Released",
                "news_post1_text": "Qwen has introduced voice and video chat capabilities in Qwen Chat, allowing users to interact with the AI like a phone or video call. The team has open-sourced Qwen2.5-Omni-7B under Apache 2.0 license, releasing both the model and technical details. This is an omni model - a single model that understands text, audio, images, and video while outputting text and audio. It features a \"thinker-talker\" architecture enabling simultaneous thinking and talking. The team believes AGI will be agent-based on omni models.",
                 // News Post 2 (Llama 4 - Now First)
                "news_post2_title": "Meta Releases Llama 4: Multimodal Capabilities Emerge",
                "news_post2_text": "Meta has announced the release of Llama 4, a new iteration in their open-source model series, notably featuring multimodal capabilities (text, image, potentially audio/video understanding). While this release expands the open-source ecosystem, initial assessments suggest it doesn't represent a fundamental leap compared to leading models like DeepSeek V3. Consequently, the progress towards AGI remains estimated at 39%. Llama 4 is a valuable contribution, but not seen as significantly altering the AGI timeline currently.",
                "news_post2_link_blog": "Official Blog Post: Llama 4 Multimodal Intelligence", // NEW link text key
                // Footer
                "footer_1": "The Road to Free Open AGI Project | Committed to ethical, open artificial intelligence for all",
                "footer_2_static": "Building tools and experiences for an open AI future."
            },
            pt: {
                 // Page Meta
                "page_title_news": "NotÃ­cias de IA - Caminho para AGI Gratuita e Aberta",
                // Navigation
                "nav_home": "Caminho para AGI", "nav_news": "NotÃ­cias de IA", "nav_courses": "Cursos", "nav_mission": "MissÃ£o", "nav_games": "Jogos", "nav_comics": "Quadrinhos", "nav_videos": "VÃ­deos", "nav_apps": "Aplicativos", "nav_contact": "Contato", "nav_about": "Sobre",
                // Shared
                "news_posted_by": "Postado por",
                "news_read_more": "Leia mais...",
                "news_read_less": "Leia menos...",
                "news_related_links": "Links Relacionados:",
                 // News Post 1 (Qwen - Now Second)
                "news_post1_title": "Qwen2.5-Omni: Primeiro Modelo Omni de CÃ³digo Aberto LanÃ§ado",
                "news_post1_text": "Qwen introduziu capacidades de chat por voz e vÃ­deo no Qwen Chat, permitindo aos usuÃ¡rios interagir com a IA como numa chamada telefÃ´nica ou de vÃ­deo. A equipe tornou open-source o Qwen2.5-Omni-7B sob a licenÃ§a Apache 2.0, liberando tanto o modelo quanto os detalhes tÃ©cnicos. Este Ã© um modelo omni - um Ãºnico modelo que entende texto, Ã¡udio, imagens e vÃ­deo, enquanto gera texto e Ã¡udio. Ele apresenta uma arquitetura \"thinker-talker\" que permite pensar e falar simultaneamente. A equipe acredita que a AGI serÃ¡ baseada em agentes sobre modelos omni.",
                 // News Post 2 (Llama 4 - Now First)
                "news_post2_title": "Meta LanÃ§a Llama 4: EmergÃªncia de Capacidades Multimodais",
                "news_post2_text": "A Meta anunciou o lanÃ§amento do Llama 4, uma nova iteraÃ§Ã£o em sua sÃ©rie de modelos de cÃ³digo aberto, notavelmente apresentando capacidades multimodais (compreensÃ£o de texto, imagem, potencialmente Ã¡udio/vÃ­deo). Embora este lanÃ§amento expanda o ecossistema de cÃ³digo aberto, avaliaÃ§Ãµes iniciais sugerem que ele nÃ£o representa um salto fundamental em comparaÃ§Ã£o com modelos lÃ­deres como o DeepSeek V3. Consequentemente, o progresso em direÃ§Ã£o Ã  AGI permanece estimado em 39%. O Llama 4 Ã© uma contribuiÃ§Ã£o valiosa, mas nÃ£o vista como alterando significativamente a linha do tempo da AGI no momento.",
                "news_post2_link_blog": "Postagem Oficial do Blog: Llama 4 InteligÃªncia Multimodal", // NEW link text key
                 // Footer
                "footer_1": "O Projeto Caminho para AGI Gratuita e Aberta | Comprometido com inteligÃªncia artificial Ã©tica e aberta para todos",
                "footer_2_static": "Construindo ferramentas e experiÃªncias para um futuro de IA aberta."
            },
            zh: {
                 // Page Meta
                "page_title_news": "AIæ–°é—» - é€šå¾€è‡ªç”±å¼€æ”¾AGIä¹‹è·¯",
                // Navigation
                "nav_home": "é€šå¾€AGIä¹‹è·¯", "nav_news": "AIæ–°é—»", "nav_courses": "è¯¾ç¨‹", "nav_mission": "ä½¿å‘½", "nav_games": "æ¸¸æˆ", "nav_comics": "æ¼«ç”»", "nav_videos": "è§†é¢‘", "nav_apps": "åº”ç”¨", "nav_contact": "è”ç³»", "nav_about": "å…³äºŽ",
                // Shared
                "news_posted_by": "å‘å¸ƒè€…ï¼š",
                "news_read_more": "é˜…è¯»æ›´å¤š...",
                "news_read_less": "æ”¶èµ·...",
                "news_related_links": "ç›¸å…³é“¾æŽ¥ï¼š",
                  // News Post 1 (Qwen - Now Second)
                "news_post1_title": "Qwen2.5-Omniï¼šé¦–ä¸ªå¼€æºOmniæ¨¡åž‹å‘å¸ƒ",
                "news_post1_text": "Qwen åœ¨ Qwen Chat ä¸­å¼•å…¥äº†è¯­éŸ³å’Œè§†é¢‘èŠå¤©åŠŸèƒ½ï¼Œå…è®¸ç”¨æˆ·åƒè¿›è¡Œç”µè¯æˆ–è§†é¢‘é€šè¯ä¸€æ ·ä¸Ž AI äº’åŠ¨ã€‚å›¢é˜Ÿå·²æ ¹æ® Apache 2.0 è®¸å¯è¯å¼€æºäº† Qwen2.5-Omni-7Bï¼Œå‘å¸ƒäº†æ¨¡åž‹å’ŒæŠ€æœ¯ç»†èŠ‚ã€‚è¿™æ˜¯ä¸€ä¸ª omni æ¨¡åž‹â€”â€”ä¸€ä¸ªèƒ½å¤Ÿç†è§£æ–‡æœ¬ã€éŸ³é¢‘ã€å›¾åƒå’Œè§†é¢‘ï¼ŒåŒæ—¶è¾“å‡ºæ–‡æœ¬å’ŒéŸ³é¢‘çš„å•ä¸€æ¨¡åž‹ã€‚å®ƒé‡‡ç”¨äº†â€œæ€è€ƒè€…-è¯´è¯è€…â€æž¶æž„ï¼Œèƒ½å¤ŸåŒæ—¶æ€è€ƒå’Œè¯´è¯ã€‚è¯¥å›¢é˜Ÿè®¤ä¸ºAGIå°†æ˜¯åŸºäºŽomniæ¨¡åž‹çš„æ™ºèƒ½ä½“ã€‚",
                 // News Post 2 (Llama 4 - Now First)
                "news_post2_title": "Metaå‘å¸ƒLlama 4ï¼šå¤šæ¨¡æ€èƒ½åŠ›æ˜¾çŽ°",
                "news_post2_text": "Meta å®£å¸ƒå‘å¸ƒ Llama 4ï¼Œè¿™æ˜¯å…¶å¼€æºæ¨¡åž‹ç³»åˆ—çš„æ–°è¿­ä»£ï¼Œæ˜¾è‘—ç‰¹ç‚¹æ˜¯å…·å¤‡å¤šæ¨¡æ€èƒ½åŠ›ï¼ˆæ–‡æœ¬ã€å›¾åƒï¼Œå¯èƒ½è¿˜æœ‰éŸ³é¢‘/è§†é¢‘ç†è§£ï¼‰ã€‚è™½ç„¶æ­¤æ¬¡å‘å¸ƒæ‰©å±•äº†å¼€æºç”Ÿæ€ç³»ç»Ÿï¼Œä½†åˆæ­¥è¯„ä¼°è¡¨æ˜Žï¼Œä¸Ž DeepSeek V3 ç­‰é¢†å…ˆæ¨¡åž‹ç›¸æ¯”ï¼Œå®ƒå¹¶æœªä»£è¡¨åŸºç¡€èƒ½åŠ›çš„æ ¹æœ¬æ€§é£žè·ƒã€‚å› æ­¤ï¼Œé€šå¾€ AGI çš„è¿›å±•ä¼°è®¡ä»ä¸º 39%ã€‚Llama 4 æ˜¯ä¸€é¡¹å®è´µçš„è´¡çŒ®ï¼Œä½†ç›®å‰æœªè¢«è§†ä¸ºèƒ½æ˜¾è‘—æ”¹å˜ AGI æ—¶é—´çº¿çš„çªç ´ã€‚",
                 "news_post2_link_blog": "å®˜æ–¹åšå®¢æ–‡ç« ï¼šLlama 4 å¤šæ¨¡æ€æ™ºèƒ½", // NEW link text key
                 // Footer
                "footer_1": "è‡ªç”±å¼€æ”¾AGIä¹‹è·¯é¡¹ç›® | è‡´åŠ›äºŽä¸ºæ‰€æœ‰äººæä¾›é“å¾·ã€å¼€æ”¾çš„äººå·¥æ™ºèƒ½",
                "footer_2_static": "ä¸ºå¼€æ”¾AIçš„æœªæ¥æž„å»ºå·¥å…·å’Œä½“éªŒã€‚"
            }
        };

        // Language management functions (Copied)
        function getStoredLanguage() {
             const storedLang = localStorage.getItem('userLanguage');
             if (storedLang && translations[storedLang]) return storedLang;
             const browserLang = navigator.language || navigator.userLanguage;
             if (browserLang.startsWith('pt')) return 'pt';
             if (browserLang.startsWith('zh')) return 'zh';
             return 'en';
        }

        function setLanguage(lang) {
             if (!translations[lang]) lang = 'en';
             localStorage.setItem('userLanguage', lang);
             document.documentElement.lang = lang;
             translatePage(lang);
        }

        function translatePage(lang) {
            if (!translations[lang]) lang = 'en';
            const langSelect = document.getElementById('languageSelect');
            if (langSelect) langSelect.value = lang;

            // Translate page title
            const pageTitleElement = document.querySelector('title[data-translate="page_title_news"]');
            if (pageTitleElement && translations[lang]["page_title_news"]) {
                pageTitleElement.textContent = translations[lang]["page_title_news"];
            }

            // Translate all elements with data-translate attribute
            document.querySelectorAll('[data-translate]').forEach(el => {
                if (el.tagName === 'TITLE') return;
                const key = el.getAttribute('data-translate');
                if (translations[lang] && translations[lang][key]) {
                     // Special check for the read-more button to use appropriate text based on state
                     if (el.classList.contains('read-more')) {
                         const postContent = el.closest('.post-content'); // Find parent content block
                         const postText = postContent?.querySelector('.post-text'); // Find text within that block
                         if (postText && postText.classList.contains('expanded')) {
                            el.textContent = translations[lang]['news_read_less'] || translations['en']['news_read_less'];
                         } else {
                            el.textContent = translations[lang]['news_read_more'] || translations['en']['news_read_more'];
                         }
                     } else {
                        el.textContent = translations[lang][key];
                     }
                }
            });

            // Translate placeholders
            document.querySelectorAll('[data-translate-placeholder]').forEach(el => { /* ... */ });
        }

        // Run on page load
        document.addEventListener('DOMContentLoaded', function() {
            // --- Initialize Language ---
            const initialLang = getStoredLanguage();
            setLanguage(initialLang); // Apply initial language

            // --- Setup Language Switcher ---
            const langSelect = document.getElementById('languageSelect');
            if (langSelect) {
                langSelect.addEventListener('change', (e) => {
                    setLanguage(e.target.value);
                });
            }

            // --- Read more functionality (Needs to work for all posts) ---
             // Get all read-more buttons *after* content is loaded
             const readMoreButtons = document.querySelectorAll('.read-more');

             readMoreButtons.forEach(button => {
                 // Find the associated elements relative to the button
                 const postContent = button.closest('.post-content');
                 if (!postContent) return; // Skip if structure is wrong

                 const postText = postContent.querySelector('.post-text');
                 const postLinks = postContent.querySelector('.post-links');

                 if (!postText || !postLinks) return; // Skip if elements not found

                 button.addEventListener('click', function() {
                     postText.classList.toggle('expanded');
                     postLinks.classList.toggle('expanded');

                     // Update button text using the current language
                     const currentLang = getStoredLanguage(); // Re-fetch current language setting
                     if (postText.classList.contains('expanded')) {
                         this.textContent = translations[currentLang]?.['news_read_less'] || translations['en']['news_read_less'];
                     } else {
                         this.textContent = translations[currentLang]?.['news_read_more'] || translations['en']['news_read_more'];
                     }
                 });
             });
             // --- End Read more ---

        }); // End DOMContentLoaded
    </script>
</body>
</html>
